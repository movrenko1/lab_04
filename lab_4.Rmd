---
title: "Лабораторная 4"
output:
  html_document:
    df_print: paged
---

#Линейные регрессионные модели 
Исходные данные: Gas mileage, horsepower, and other information for 392 vehicles.
Рассмотреть модели с категориальными предикторами, включая их взаимодействие с непрерывными объясняющеми переменными.
```{r first}
library('ISLR')
library('GGally')       # графики совместного разброса переменных
library('lmtest')       # тесты остатков регрессионных моделей
library('FNN')          # алгоритм kNN
library('mlbench')

my.seed <- 12345
train.percent <- 0.85
data(Auto)            
Auto <- Auto[,-c(3,4,8,9)]
Auto$cylinders <- as.factor(Auto$cylinders)

set.seed(my.seed)
inTrain <- sample(seq_along(Auto$mpg), 
                  nrow(Auto) * train.percent)
df.train <- Auto[inTrain, c(colnames(Auto)[-1], colnames(Auto)[1])]
df.test <- Auto[-inTrain, -1]
summary(df.train)
```

```{r second}
# совместный график разброса переменных
ggp <- ggpairs(df.train)
print(ggp, progress = F)

# цвета по фактору cylinders
ggp <- ggpairs(df.train, mapping = ggplot2::aes(color = cylinders))
print(ggp, progress = F)
```

Судя по коробчатой диаграмме на пересечении cylinders и weight, у восьмицилиндрованного двигателя вес автомобиля больше. Потребления топлива двигателем транспортного средства у 4-х и 5-и цилиндрованного двигателя практически одинакова, но посравнению с дурими значительно больше.

#Модели
```{r third}
model.1 <- lm(mpg ~  cylinders + weight + acceleration+year,
              data = df.train)
summary(model.1)
model.2 <- lm(mpg ~ cylinders + weight +year,
              data = df.train)
summary(model.2)
```

acceleration - переменная оказалась незначимой. Теперь model.2 все объясняющие переменные значимы.

#Проверка остатков 
```{r fourth}
# Проверка остатков  
# тест Бройша-Пагана
bptest(model.2)
# статистика Дарбина-Уотсона
dwtest(model.2)
# графики остатков
par(mar = c(4.5, 4.5, 2, 1))
par(mfrow = c(1, 3))
plot(model.2, 1)
plot(model.2, 4)
plot(model.2, 5)
par(mfrow = c(1, 1))
```

Судя по графику слева, остатки не случайны, и их дисперсия непостоянна.В модели есть три влиятельных наблюдения: 326, 275, 167, – которые, однако, не выходят за пределы доверительных границ на третьем графике. Графики остатков заставляют усомниться в том, что остатки удовлетворяют условиям Гаусса-Маркова.


#Сравнение с KNN
```{r fifth}
# фактические значения y на тестовой выборке
y.fact <- Auto[-inTrain, 1]
y.model.lm <- predict(model.2, df.test)
MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)
# kNN требует на вход только числовые переменные
df.train.num <- as.data.frame(apply(df.train, 2, as.numeric))
df.test.num <- as.data.frame(apply(df.test, 2, as.numeric))
for (i in 2:50) {
  model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% 'mpg')], 
                       y = df.train.num[, 'mpg'], 
                       test = df.test.num, k = i)
  y.model.knn <- model.knn$pred
  if (i == 2) {
    MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)
  } else {
    MSE.knn <- c(MSE.knn, 
                 sum((y.model.knn - y.fact)^2) / length(y.model.knn))
  }
}
# график
par(mar = c(4.5, 4.5, 1, 1))
# ошибки kNN
plot(2:50, ylim = c(0,50), MSE.knn, type = 'b', col = 'darkgreen',
     xlab = 'значение k', ylab = 'MSE на тестовой выборке')
# ошибка регрессии
lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2)
legend('bottomright', lty = c(1, 2), pch = c(1, NA), 
       col = c('darkgreen', grey(0.2)), 
       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 
       lwd = rep(2, 2))

```
Как можно видеть по графику, ошибка регрессии на тестовой выборке больше, чем ошибка метода k ближайших соседей. Далее с увеличением количества соседей точность kNN  практически не изменяется, данная модель пригодна для прогнозирования. 
